[
["index.html", "Teorijski priručnik za LSM Predgovor", " Teorijski priručnik za LSM Blagoje Ivanović Poslednja izmena: 31 December, 2018 Predgovor Na ovoj stranici će biti prikazane teorijske osnove potrebne za praćenje vežbi i razumevanje kodova koji se mogu naći na stranici kursa. Može se shvatiti kao beleške sa vežbi. U razvojnoj je fazi i biće dopunjavana relativno često. Opširnije teorijsko razmatranje linearnih modela se nalazi u materijalima profesorke Bojane Milošević, na adresi: http://www.matf.bg.ac.rs/p/bojana-milosevic/kurs/533/linearni-statisticki-modeli/ Sa leve strane se nalazi sadržaj koji sadrži oblasti koje su predjene na vežbama (za koje je napisan teorijski uvod). Kodovi u R-u koji prate sve oblasti se nalaze na http://www.matf.bg.ac.rs/p/blagoje-ivanovic/cas/2731/%C4%8Casovi/ "],
["analiza-glavnih-komponenti-pca.html", "1 Analiza glavnih komponenti (PCA) 1.1 Ideja i postupak 1.2 Smanjivanje dimenzije Neke napomene", " 1 Analiza glavnih komponenti (PCA) Kod višestruke regresije u opštem slučaju prediktori su korelisani. Analiza glavnih komponenti (Principal Component Analysis) koristi se kao alat u ispitivanju podataka i pri pravljenju modela; u pretprocesiranju, ispitivanju skupa opservacija i skupa prediktora. Ovom statističkom procedurom se, korišćenjem linearnih transformacija, početni skup prediktora transformiše u novi skup nekorelisanih prediktora. Pritom se identifikuju oni prediktori koji su najbitniji za podatke, koji opisuju najviše varijabilnosti, i oni koji nisu toliko bitni. Te nebitne prediktore onda možemo izbaciti i sprovesti linearnu regresiju sa transformisanim prediktorima i sa manje njih. Na taj način se PCA koristi za smanjivanje dimenzionalnosti i povećanje interpretabilnosti. (Uvod kopiran sa odlične prezentacije Anice Kostić (link), koju valja pogledati za detaljniji pregled PCA) 1.1 Ideja i postupak Pretpostavimo da imamo sledeći skup podataka (samo prediktori). # Generisemo neke podatke set.seed(216) x1 &lt;- rnorm(100) x2 &lt;- x1 + rnorm(100, sd = 0.5) # Vidimo da je najveca varijabilnost podataka duz prave y = x par(pty=&quot;s&quot;) # hocemo kvadratni grafik plot(x1, x2, xlim = c(-3, 3), ylim = c(-3, 3)) arrows(c(mean(x1) - 1, mean(x1) + 1), c(mean(x2) - 1, mean(x2) - 1), c(mean(x1) + 1, mean(x1) - 1), c(mean(x2) + 1, mean(x2) + 1), col=&quot;blue&quot;, lwd = 2) Strelicama su označeni pravci najveće varijabilnosti u podacima. Analiza glavnih komponenti se vrši nad standardizovanim podacima, pa pretpostavljamo da je \\(X\\) ovde matrica standardizovanih prediktora. Osnovna ideja je da se rotira ovaj skup prediktora tako da se pravci najvece varijabilnosti nalaze na osama i da prediktori postanu ortogonalni. Rotacija se dobija množenjem matricom, pa uzimamo nov skup prediktora \\(Z = XU\\), gde treba odrediti \\(U\\) tako da prediktori \\(Z\\) budu ortogonalni, tj. da je \\(Z^\\top Z\\) dijagonalna. Matrica \\(X\\top X\\) je simetrična, pa se može dijagonalizovati, odnosno postoji ortogonalna matrica \\(M\\) (sačinjena od sopstvenih vektora matrice \\(X\\top X\\)), takva da je \\(X\\top X = MDM^\\top\\), gde je \\(D=diag\\{\\lambda_1, \\dots, \\lambda_p\\}\\) dijagonalna matrica sa sopstvenim vrednostima matrice \\(X\\top X\\) na dijagonali. Odatle, ako uzmemo da je \\(U=M\\), tj. \\(Z = XM\\), imamo \\[Z\\top Z = M\\top X\\top XM=M\\top MDM^\\top M = D,\\] jer je \\(M\\) ortogonalna. Dakle, ako je \\(U\\) matrica čije su kolone sopstveni vektori matrice \\(X\\top X\\), transformisani prediktori \\(Z=XU\\) će biti ortogonalni. Time dobijamo postupak analize glavnih komponenti: Standardizujemo prediktore i uzmemo matricu standardizovanih prediktora \\(X\\) Odredimo sopstvene vektore matrice \\(X\\top X\\) Napravimo matricu \\(U\\) čije su kolone sopstveni vektori matrice \\(X\\top X\\) Uzmemo nov sistem prediktora \\(Z=XU\\) Glavne komponente su kolone matrice \\(Z\\) 1.2 Smanjivanje dimenzije Jedna od najbitnijih upotreba za PCA je smanjivanje dimenzije. Naime, ako imamo mnogo prediktora, korisno je smanjiti broj prediktora sa kojima se radi analiza, radi interpretabilnosti. Medjutim, nekad se ne mogu prosto izbaciti prediktori iz modela, već neke linearne kombinacije istih utiču na model. Analiza glavnih komponenti teži da nadje ove linearne kombinacije prediktora iz \\(X\\) koje najviše doprinose varijabilnosti podataka. Često se nakon analize glavnih komponenti koristi prvih nekoliko glavnih komponenti, čime se smanjuje broj korišćenih prediktora za modelovanje. Neko okvirno pravilo je da se uzme onoliko komponenti koje objašnjavaju više od 80% varijabilnosti u podacima, što se meri veličinom sopstvenih vrednosti (prve glavne komponente odgovaraju najvećim sopstvenim vrednostima). Takodje, nekad je moguće na osnovu razmatranja matrice rotacije \\(U\\) odrediti tačno koje linearne kombinacije prediktora su bitne. Na primer, ako je matrica rotacije približno \\(U=\\begin{pmatrix}1 &amp; 0 \\\\ -1 &amp; 0\\end{pmatrix}\\), to znači da je prva glavna komponenta \\(X_1 - X_2\\), pa ako je udeo varijabilnosti objašnjen prvom glavnom komponentom veoma veliki, dobar model bi mogao da koristi samo jedan prediktor, i to \\(X_1 - X_2\\), čime smo modelovanje sveli na prostu regresiju. Neke napomene Analiza glavnih komponenti ne uzima u obzir zavisnu promenljivu, već se bavim samo transformacijama prostora prediktora. Izuzetno je bitno standardizovati podatke pre analize glavnih komponenti. Ako se podaci ne standardizuju, problem koji se može javiti je da prediktor koji uzima vrednosti u intervalu [-100, 100] izgleda da doprinosi varijabilnosti mnogo više nego neki koji je na [-0.1, 0.1], iako su ravnopravni kad se skaliraju. Vrlo slikovito objašnjenje analize glavnih komponenti se može videti na sajtu Cross Validated "],
["logisticka-regresija.html", "2 Logistička regresija 2.1 Uvod 2.2 Kvote 2.3 Ocene parametara \\(\\beta\\) 2.4 Interpretacija parametara \\(\\beta\\) 2.5 Testiranje značajnosti koeficijenata", " 2 Logistička regresija 2.1 Uvod Pretpostavimo da je zavisna promenljiva \\(Y\\) indikator nekog dogadjaja, tj. da ima Bernulijevu raspodelu, čiji parametar zavisi od vrednosti prediktora \\(X\\): \\[Y:\\begin{pmatrix}0 &amp; 1 \\\\1-\\pi(X) &amp; \\pi(X)\\end{pmatrix}.\\] Kako \\(Y\\) uzima samo dve vrednosti, nema smisla koristiti linearan model \\(Y = \\beta_0 + \\beta_1X\\). Smislenije je modelovati verovatnoću \\(P\\{Y=1\\mid X\\} = \\pi(X)\\in[0,1]\\), ali, zbog ograničenosti verovatnoće, model \\(\\pi(X)=\\beta_0 + \\beta_1X\\) takodje nije adekvatan. Zato je potrebno modelovati ovu verovatnoću kao \\(\\pi(X) = g(\\beta_0 + \\beta_1X)\\), gde je \\(g\\) funkcija koja uzima vrednosti iz \\([0,1]\\). Postoje razni izbori ove funkcije \\(g\\), od kojih su najčešći funkcije raspodela odredjenih raspodela verovatnoća (npr. probit regresija koristi standardnu normalnu raspodelu \\(g(x)=\\Phi(x)\\)). U kontekstu logističke regresije, model koji koristimo je \\[\\pi(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}.\\] U ovom slučaju uzeli smo \\(g(x) = \\frac{e^x}{1+e^x}\\), što je funkcija raspodele logističke raspodele. Ova funkcija se takodje zove i logistička funkcija, a i specijalan slučaj je tzv. sigmoidnih funkcija. U slučaju da imamo više prediktora, model se jednostavno proširuje na \\[\\pi(X) = \\frac{e^{X\\beta}}{1+e^{X\\beta}},\\] gde je \\(\\beta\\) vektor parametara a \\(X\\) model matrica. 2.2 Kvote Ako je \\(\\pi(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}\\), onda će biti \\[\\log\\frac{\\pi(X)}{1-\\pi(X)} = \\beta_0 + \\beta_1X.\\] Veličina \\(\\frac{\\pi(X)}{1-\\pi(X)}\\in(0, +\\infty)\\) se naziva kvotom i mi u logističkoj regresiji u suštini modelujemo logaritam kvote linearnim modelom. Kvota \\(q(X) = \\frac{\\pi(X)}{1-\\pi(X)}=\\frac{P\\{Y=1\\mid X\\}}{P\\{Y=0\\mid X\\}}\\) predstavlja odnos verovatnoća da zavisna promenljiva uzme vrednost 1 odnosno 0, za odredjenu vrednost prediktora. Ukoliko su ove dve verovatnoće jednake, kvota će biti 1. Ako je \\(q(X)&gt;1\\), to znači da je verovatnoća da \\(Y\\) uzme vrednost 1 \\(q(X)\\) puta veča nego da uzme vrednost 0. Obrat važi u slučaju da je \\(q(X)&lt;1\\). Na primer, ako je \\(q(X)=4=4:1\\), to znači da je 4 puta verovatnije da je \\(Y=1\\) nego \\(Y=0\\). S druge strane, ako je \\(q(X)=0.125=1:8\\), to znači da je 8 puta verovatnije da je \\(Y=0\\) nego \\(Y=1\\). 2.3 Ocene parametara \\(\\beta\\) Parametri modela logističke regresije se ocenjuju metodom maksimalne verodostojnosti, odnosno rešavanjem minimizacionog problema (zbog lakšeg računa se gleda logaritam funkcije verodostojnosti): \\[\\hat\\beta = \\underset{\\beta}{argmin}\\ \\log\\left( \\prod_{i=1}^n\\left(1-\\pi(x_i)\\right)^{1-y_i}\\pi(x_i)^{y_i}\\right).\\] Detalji izračunavanja se nalaze u skripti za predavanja. 2.4 Interpretacija parametara \\(\\beta\\) Kod (proste) linearne regresije, parametar \\(\\beta_1\\) se interpretira kao “za koliko se promeni \\(Y\\) ako se \\(X\\) promeni za 1 jedinicu mere”. Kod logističke regresije je situacija nešto kompleksnija, jer imamo logističku transformaciju. Pogledajmo šta se dešava ako \\(X\\) povećamo za 1 jedinicu mere. Kako je \\(\\log(q(X)) = \\beta_0 + \\beta_1X\\), biće \\(\\log(q(X+1)) - \\log(q(X)) = \\beta_1\\), tj. \\(\\beta_1\\) predstavlja promenu logaritma kvote. Odavde imamo \\(\\log\\frac{q(X+1)}{q(X)}= \\beta_1\\), pa stoga \\[\\frac{q(X+1)}{q(X)} = e^\\beta_1.\\] Dakle, prilikom povećanja vrednosti \\(X\\) za 1 jedinicu mere, kvota se menja \\(e^\\beta_1\\) puta. Ako je \\(\\beta_1=0\\), to znači da nema promene u kvoti, a samim tim ni u verovatnoći \\(\\pi(X)\\). Ovo je u saglasnosti sa slučajem linearne regresije kada smo testirali da li je koeficijent nula, čime smo odredjivali da li je značajan ili ne. Ako je \\(\\beta_1&lt;0\\), kvota se smanjuje, dok ako je \\(\\beta_1&gt;0\\) kvota se povećava. Kako se povećanjem (smanjenjem) kvote povećava (smanjuje) i verovatnoća \\(\\pi(X)\\), zaključujemo da znak koeficijenta \\(\\beta_!\\) odredjuje pozitivan ili negativan uticaj prediktora \\(X\\) na verovatnoću \\(P\\{Y=1\\mid X\\}\\). Konkretna vrednost \\(e^\\beta_1\\) odredjuje koliko puta se menja kvota jediničnom promenom u prediktoru \\(X\\). Ova priča je bila samo u slučaju 1 prediktora \\(X\\), ali se jednostavno uopštava na više prediktora. Naime, koeficijent uz \\(X_1\\) će uticati na promenu u kvoti za jedinično povečanje vrednosti \\(X_1\\), pri uslovu da su vredosti svih drugih prediktora fiksirane. 2.5 Testiranje značajnosti koeficijenata 2.5.1 Valdov test Kao što smo rekli, smatramo da postoji uticaj prediktora \\(X\\) na verovatnoću \\(\\pi(X)\\) i kažemo da je statistički značajan, ako je \\(\\beta_1\\neq 0\\). Dakle, prilikom testiranja značajnosti nekog koeficijenta \\(\\beta_i\\) testiraćemo hipotezu \\[H_0 : \\beta_i=0\\quad \\text{protiv} \\quad H_1:\\beta_i\\neq0.\\] U ovu svrhu možemo koristiti test statistiku: \\[\\frac{\\hat{\\beta_i} - \\beta_i}{s.e.(\\hat\\beta_i)} \\sim \\mathcal{N}(0,1),\\] što znači da pod nultom hipotezom \\(H_0 : \\beta_i=0\\) važi \\[\\frac{\\hat{\\beta_i}}{s.e.(\\hat\\beta_i)} \\sim \\mathcal{N}(0,1).\\] Ovaj test je korišćen u R-u za testiranje značajnosti pojedinačnih koeficijenata (zvezdice). 2.5.2 Test količnika verodostojnosti Ukoliko imamo dva ugnježdjena modela \\(M_A\\) i \\(M_B\\), gde je \\(M_A\\) “manji”, tj. \\(M_A\\) se dobija postavljanjem nekih koeficijenata \\(M_B\\) na nulu, možemo proveriti da li postoji statistički značajna razlika izmedju njih koristeći test količnika verodostojnosti. Ako pretpostavimo da model \\(M_A\\) ima \\(p_A\\) koeficijenata, a \\(M_B\\) \\(p_B\\) koeficijenata, koristimo test statistiku \\[2(\\log L(\\hat\\beta_{B})-\\log L(\\hat\\beta_{A})) = 2\\log\\left(\\frac{L(\\hat\\beta_{B})}{L(\\hat\\beta_{A})}\\right)\\sim \\chi_{p_B-p_A}^2,\\] gde je \\(L(\\beta)\\) funkcija verodostojnosti. Velike vrednosti ove statistike ukazuju na postojanje značajne razlike medju modelima. U R-u je ovaj test implementiran u funkciji anova kada joj se da argument test = &quot;Chisq&quot;. Vidimo da ovim testom možemo da napravimo analogon \\(F\\) testu za linearni model, poredeći odnos verodostojnosti modela koji koristimo i modela koji ujljučuje samo slobodan član. Treba imati u vidu da će p-vrednosti testiranja značajnosti pojedinačnih koeficijenata Valdovim testom i testom količnika verodostojnosti u opštem slučaju biti različite (za razliku od \\(t\\) i \\(F\\) testa u slučaju linearnih modela). "],
["puasonova-regresija.html", "3 Puasonova regresija", " 3 Puasonova regresija Kod Puasonove regresije pretpostavljamo da zavisna promenljiva ima Puasonovu raspodelu \\[Y \\sim \\mathcal{P}(\\lambda(X)),\\] gde parametar raspodele \\(\\lambda\\) zavisi od prediktora \\(X\\). Kako zbog prirode zavisne promenljive, koja uzima celobrojne vrednosti, nije pogodno modelovati \\(Y\\) linearnim modelom, mi ćemo modelovati očekivanje \\(E(Y|X) = \\lambda(X)\\), koje uzima realne vrednosti. Kako je parametar \\(\\lambda\\) pozitivan, modelovaćemo njegov logaritam linearnim modelom, čime dobijamo model Puasonove regresije \\[\\log(\\lambda(X)) = \\beta_0 + \\beta_1X_1+\\dots+\\beta_pX_p=X\\beta,\\] gde je \\(\\beta\\) vektor koeficijenata modela. Ocene koeficijenata \\(\\beta\\) se računaju metodom maksimalne verodostojnosti. naime \\[\\hat\\beta = \\underset{\\beta}{argmin}\\ \\log\\left( \\prod_{i=1}^n \\frac{\\lambda(x_i)^{y_i}}{y_i!}e^{-\\lambda(x_i)}\\right).\\] Testiranje značajnosti koeficijenata se radi na isti način kao u logističkoj regresiji, pa nećemo ulaziti u detalje ovde. "]
]
